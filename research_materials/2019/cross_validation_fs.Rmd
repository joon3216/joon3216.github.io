---
title: "Cross-validation for fusion estimates"
header-includes:
 - \usepackage{cancel}
 - \newcommand{\bX}{\mathbf{X}}
 - \newcommand{\bx}{\mathbf{x}}
 - \newcommand{\bY}{\mathbf{Y}}
author: Junkyu Park
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    includes:
      in_header: 
        - ../../style/all_ga_script.html
        - ../../style/all_navbar_head.html
        - ../../style/all_orange_jp_02_lvl.html
      before_body:
        - ../../style/all_navbar_body_02_lvl.html
      after_body:
        - ../../style/all_footer.html
    toc: TRUE
    self_contained: FALSE
---


The following link is pointing towards this document:

* [Dealing with a non-separable penalty term](../2018/non_separable_penalty.html)

The following external R packages/functions are used:

```{r externals, warning = F, message = F}
library(dplyr)
library(ggplot2)
gather <- tidyr::gather
```


This note is a continuation from [Dealing with a non-separable penalty term](../2018/non_separable_penalty.html) under 2018: Statistical Computation.


# 1. Introduction

This note concerns with a cross-validation method. In particular, it is interested in finding an optimal tuning parameter $\lambda$ and a corresponding $\boldsymbol{\theta}$ that yields a lowest cross-validation error. The note will demonstrate how the cross-validation is used to find $\lambda$, and provide functions that help achieve the goal. 


# 2. Choosing a tuning parameter

From the [previous note](../2018/non_separable_penalty.html), I assumed: $$y_t = \theta_t + \varepsilon_t$$ with $\varepsilon_t \stackrel{iid}{\sim} N(0, \sigma^2)$, and used the cost function that has a penalty: $$\text{cost}(\theta_1, \phi_2, \dots, \phi_n) = \sum_{t = 1}^{n} (y_t - \theta_t)^2 + \lambda \sum_{t = 2}^{n} |\phi_t|$$ where $\phi_t = \theta_t - \theta_{t - 1}$. 

To find an optimal $\lambda$, we shall use the following strategy:

1. Divide $y_t$'s into $k$ folds where the $s$^th^ fold is defined as $\text{Fold}_s := \{ y_{t} \}_{t \in A_s}$, where $A_s = \{t \leq n \text{ | } t = s + kt^*, t^* \geq 0 \}$.
2. Given $\lambda$, compute the estimates $\boldsymbol{\theta}_s$'s based on $\{y_t \}_{t \notin \text{Fold}_s}$ for all $s = 1, \dots, k$.
4. Define $\text{interpolate}_{\boldsymbol{\theta}_s}(t)$ for each $s$, an interpolating function based on $\boldsymbol{\theta}_s$.
3. Compute $\text{loss}_s$ for each $s$ where $$\text{loss}_s := \frac{1}{|\text{Fold}_s|} \sum_{t \in \text{Fold}_s} (\text{interpolate}_{\boldsymbol{\theta}_s}(t) - y_t)^2$$
4. Compute $\text{error} := \frac{1}{k}\sum_{s = 1}^{k} \text{loss}_s$.
5. Choose $\lambda$ that yields a minimum $\text{error}$.

Step 1 is performed by `split_data_1d`:

```{r split_data}
split_data_1d <- function(y, num_folds, fold) {
    # y: a numeric vector
    # num_folds: a natural number <= number of observations, e.g. 5
    # fold: a natural number <= num_folds, e.g. 3
    
    n <- length(y)
    indices <- 1:n
    assign_folds <- rep(1:num_folds, n %/% num_folds + 1)[indices]
    partition <- split(indices, assign_folds)
    fold_indices <- partition[[fold]]
    y_fold <- y[fold_indices]
    y_rest <- y[-fold_indices]
    list(
        y_fold = list(
            data = y_fold,
            indices = fold_indices
        ), 
        y_rest = list(
            data = y_rest, 
            indices = (1:n)[!(1:n %in% fold_indices)]
        )
    )
}
```

Step 2 is performed by `fusion_estimates`, a function defined in the [previous note](../2018/non_separable_penalty.html):

```{r fusion_estimates}
fusion_estimates <- function(y, theta, lambda, max_iter = 1000, eps = 1e-5) {
    n <- length(y)
    if (missing(theta)) {theta <- y}
    if (length(theta) != n) {
        stop(paste0(
            '\nError in fusion_estimates():\n', 
            'The length of given initial theta is ', length(theta),
            ', which is not equal to length(y) == ', n, '.'
        ))
    }
    phi <- diff(theta)
    phisums_old <- cumsum(phi)
    theta_1_new <- (sum(y) - sum(phisums_old)) / n
    cost <- sum((y - theta)^2) + lambda * sum(abs(phi))
    costs <- NULL
    costs[1] <- cost # costs
    there_is_a_progress <- T
    iter <- 0
    while (there_is_a_progress & iter < max_iter) {
        # Store new phi_1 (= 0) to phi_n in phi_new
        phi_new <- numeric(length = n) 
        for (j in 2:n) {
            phisums_new <- cumsum(phi_new)
            req <- sum(
                phisums_old[(j - 1):(n - 1)] - 
                phisums_old[j - 1] + phisums_new[j - 1]
            )
            discri <- sum(y[j:n]) - (n - j + 1) * theta_1_new - req
            if (discri < -lambda / 2) {
                phi_new[j] <- (discri + lambda / 2) / (n - j + 1)
            } else if (discri > lambda / 2) {
                phi_new[j] <- (discri - lambda / 2) / (n - j + 1)
            } # already 0 otherwise
        }
        phi_new <- phi_new[-1]
        phisums_new <- phisums_new[-1]
        theta <- c(theta_1_new, theta_1_new + phisums_new)
        cost <- sum((y - theta)^2) + lambda * sum(abs(phi_new))
        theta_1_new <- (sum(y) - sum(phisums_new)) / n
        phisums_old <- phisums_new
        iter <- iter + 1
        costs[iter + 1] <- cost
        there_is_a_progress <- !(abs(costs[iter] - cost) <= eps)
    }
    list(
        theta = theta, 
        phi = phi_new, 
        lambda = lambda, 
        iter = iter, 
        costs = costs # the first cost is calculated at iteration 0
    )
}
```

```{r example01, echo = F, eval = F}
split_y <- split_data_1d(y, 5, 2)
indices <- split_y$y_rest$indices
fusion_y <- fusion_estimates(split_y$y_rest$data, lambda = 1)
theta <- fusion_y$theta
qplot(indices, theta)
```

Step 3 is done by `interpolate_1d`:

```{r train_model}
interpolate_1d <- function(t, theta_rest, indices_rest, n) {
    # t: a number, or a numeric vector whose minimum is at least 1 and
    #    whose maximum is at most n
    # theta_rest: a numeric vector
    # indices_rest: an integer vector of length length(theta_rest)
    # n: an integer; the length of "full" data
    
    indices <- sort(indices_rest)
    if (max(t) > n || min(t) < 1) {
        stop(paste0(
            '\nError in interpolate_1d():\n', 
            'Extrapolation not available;\n',
            'either max(t) > length of full data or min(t) < 1 happened'
        ))
    }
    if (length(theta_rest) != length(indices)) {
        stop(paste0(
            '\nError in interpolate_1d():\n', 
            'length(theta_rest) != length(indices)'
        ))
    }
    sapply(t, function(d){theta_rest[which.min(abs(d - indices))]})
}
```

$\text{loss}_s$ in step 4 is computed with `loss_1d_fold`:

```{r loss_1d_fold}
loss_1d_fold <- function(y, theta, indices_fold) {
    # y: a numeric vector of length n
    # theta: a numeric vector of length < n
    # indices_fold: an integer vector of length n - length(theta)
    
    n <- length(y)
    indices_fold <- sort(indices_fold)
    y_fold <- y[indices_fold]
    indices_rest <- (1:n)[!(1:n %in% indices_fold)]
    interpolate <- interpolate_1d(indices_fold, theta, indices_rest, n)
    mean((interpolate - y_fold)^2)
}
```

and for the sake of computing training error, we define `loss_1d` as well:

```{r loss_1d}
loss_1d <- function(y, theta) {
    # y: the same as in loss_1d_fold
    # theta: a numeric vector of length n
    
    mean((y - theta)^2)
}
```


```{r example02, echo = F, eval = F}
# y # length == 1000
# theta # length == 800
# indices_fold # length == 200
loss_1d_fold(y, theta, indices_fold)
```

We shall now compute the cross-validation error in `cv_error_1d`:

```{r cv_error_1d}
cv_error_1d <- function(y, k, lambda) {
    # y: a numeric vector
    # k: an integer; a number of folds
    # lambda: a positive real number
    
    losses <- numeric(length = k)
    for (s in 1:k) {
        split_s <- split_data_1d(y, k, s)
        fusion_s <- fusion_estimates(split_s$y_rest$data, lambda = lambda)
        losses[s] <- loss_1d_fold(y, fusion_s$theta, split_s$y_fold$indices)
    }
    list(error = mean(losses), losses = losses)
}
```

```{r example03, echo = F, eval = F}
cv_error_1d(y, 5, 1)
cv_error_1d(y, 5, 2)
```


# 3. An example

The example in the previous note is regenerated:

```{r regenerate, fig.asp = .6, fig.align = 'center'}
set.seed(1024)
n <- 1000
t <- 1:n
f <- function(t) {t / 250 - .5}
g <- function(t) {-(.25 / 449) * t + 250 / 449}
true_theta <- c(rep(0, 249), f(250:500), rep(.75, 50), g(551:1000))
y <- true_theta + rnorm(1000, 0, 0.1)
qplot(t, y)
```

The following `lambda` values will be considered. Also, say $k = 5$, an another arbitrary choice:

```{r lambdas_to_publish, eval = F}
lambdas <- seq(.1, 5, by = .1)
k <- 5
```

For each `lambda`, let's compute the training error and the cross-validation error:

```{r computation, eval = F}
start <- Sys.time()
errors <- t(sapply(
    lambdas,
    function(l) {
        c(
            cv_error_1d(y, k, l)$error,
            loss_1d(y, fusion_estimates(y, lambda = l)$theta)
        )
    }
)) %>%
    'colnames<-'(c(paste0('C-V error, ', k, ' folds'), 'Training')) %>%
    cbind(lambdas = lambdas) %>%
    as_tibble()
end <- Sys.time()
```

This process takes a while. Here's the link to the `errors` [csv file](../../files/cv_errors.csv).

```{r end_start, eval = F}
end - start
```

```{r lambdas, echo = F}
end2 <- as.POSIXct("2019-05-09 22:32:29 EDT")
start2 <- as.POSIXct("2019-05-09 21:57:49 EDT")
end2 - start2
```

The visualization is as follows:

```{r errors, echo = F, warning = F, message = F}
errors <- readr::read_csv('../../files/cv_errors.csv')
```


```{r visualize_errors, fig.asp = .6, fig.align = 'center'}
errors %>%
    gather(Errors, value, -lambdas) %>%
ggplot(aes(x = lambdas, y = value, col = Errors)) +
    geom_point(aes(shape = Errors)) +
    geom_line() +
    labs(x = 'Lambda', y = 'Average squared error') +
    theme(legend.position = 'top')
```

The training error increases as lambda increases. This makes sense since, as shown in the [previous note](../2018/non_separable_penalty.html), $\theta_t \to \overline{y}$ for all $t$ as $\lambda$ gets greater, and $\theta_t = \overline{y}$ for all $t$ starting from a certain value of $\lambda$. That is, as lambda increases, the fusion estimates start to move away from the least squares estimates $\hat{\theta}_t$'s ($= y_t$ for all $t$), which are estimates when $\lambda = 0$ and are values that minimize the average squared error, so the training error increases as shown in the plot.


The minimum lambda that yields the lowest cv-error is therefore:

```{r conclusion}
errors %>%
    gather(Errors, value, -lambdas) %>%
    filter(Errors != 'Training') %>%
    filter(value == min(value)) %>%
    pull(lambdas)
```

The proposed value of $\lambda$ is `r 1`.

# Session info

R session info:

```{r session_info}
sessionInfo()
```








