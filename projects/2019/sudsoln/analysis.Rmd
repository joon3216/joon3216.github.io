---
title: "sudsoln: analysis"
author: Junkyu Park
output: 
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    includes:
      in_header: 
        - ../../../style/all_ga_script.html
        - ../../../style/all_navbar_head.html
        - ../../../style/all_orange_jp_03_lvl.html
      before_body:
        - ../../../style/all_navbar_body_03_lvl.html
      after_body:
        - ../../../style/all_footer_03_lvl.html
    toc: FALSE
    self_contained: FALSE
---

```{r setup, echo = F, results = 'hide'}
library(reticulate)
knitr::knit_engines$set(python = eng_python)
use_virtualenv('r-reticulate')
py_available(T)
```

The following packages are used for analysis:

```{python packages_loaded}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import sudsoln as ss
```

```{python ss_ver, echo = F}
ss_ver = ss.__version__
```

```{r ss_ver_to_r, echo = F}
ss_ver = py$ss_ver
```

`sudsoln.Sudoku.solve()` method is used to solve [95 hard](https://norvig.com/top95.txt) questions and [11 hardest](https://norvig.com/hardest.txt) questions posted on Dr. Peter Norvig's [website](https://norvig.com/sudoku.html). 


<!--html_preserve-->
<div id="TOC" class="toc" style="padding:2rem 2rem 0 0;">
<ul style="list-style:none;">
<li><a href="#results">1. Results</a></li>
<li><a href="#analysis">2. Analysis</a><ul>
<li><a href="#how-long-did-it-take-to-solve-a-puzzle">2.1. How long did it take to solve a puzzle?</a></li>
<li><a href="#how-many-puzzles-required-a-brute-force-to-be-solved">2.2. How many puzzles required a brute force to be solved?</a></li>
<li><a href="#how-many-attempts-did-it-take-to-solve-a-puzzle-if-forcefully-solved">2.3. How many attempts did it take to solve a puzzle if forcefully solved?</a></li>
</ul></li>
<li><a href="#session-info">Session info</a></li>
</ul>
</div>
&nbsp;
<!--/html_preserve-->


# 1. Results

You can run the following [script](../../../files/sudsoln_solving.py) to create [result_sudsoln`r ss_ver`.csv](../../../files/result_sudsoln`r ss_ver`.csv) file, and then this [script](../../../files/sudsoln_produce_results.py) to produce tables used in analysis. Codes that produced plots in [section 2.3](#how-many-attempts-did-it-take-to-solve-a-puzzle-if-forcefully-solved) are written [here](../../../files/sudsoln_produce_plots.py).

`time`s are in seconds:

```{python produce_results, echo = F, results = 'hide'}
import os
os.chdir('../../../files')
exec(open('sudsoln_produce_results.py').read())
os.chdir('../projects/2019/sudsoln')
```

```{python result_sudsoln_report1, echo = F}
print(result_sudsoln_report1)
all_is_solved = result_sudsoln_report1.iloc[2, 0]
all_total = result_sudsoln_report1.iloc[2, 1]
```

```{r result_sudsoln_report1_py, echo = F}
all_is_solved <- py$all_is_solved
all_total <- py$all_total
```

Note that:

1. `r all_is_solved` out of `r all_total` are solved, which results in a solving rate of `r round(all_is_solved / all_total * 100, 3)`%. I'm hoping to achieve 100% solution rate in future updates by incorporating more logical approaches and adapting a better search algorithm.
2. `time`s are extremely right-skewed in every `category`. See [section 2.3.](#how-many-attempts-did-it-take-to-solve-a-puzzle-if-forcefully-solved) below for details.

# 2. Analysis

## 2.1. How long did it take to solve a puzzle?

`time`s displayed in the table above show how long `ss.Sudoku.solve()` ran most of the time, NOT how long it took to solve a puzzle. To see how long the method ran most of the time *given that it solved a puzzle*, a grouping by `is_solved` column is required:

```{python result_sudsoln_report2, echo = F}
print(result_sudsoln_report2)
top95_true_median = result_sudsoln_report2.iloc[0, 2]
top95_true_avg = result_sudsoln_report2.iloc[0, 3]
hardest_true_avg  = result_sudsoln_report2.iloc[2, 3]
hardest_true_max  = result_sudsoln_report2.iloc[2, 4]
```


```{r result_sudsoln_report2_py, echo = F}
top95_true_median <- py$top95_true_median
top95_true_avg <- py$top95_true_avg
hardest_true_avg <- py$hardest_true_avg
hardest_true_max <- py$hardest_true_max
```

In terms of times consumed, it is hard to say there is a difference in terms of difficulty between puzzles in `top95` category and `hardest` category. One reason for this is because there are only 11 puzzles in `hardest`. It is true that every solved puzzles in `hardest` category are solved in `r round(hardest_true_max, 3)` seconds, which is less than the average time (`r round(top95_true_avg, 3)` seconds) consumed by solved puzzles in `top95` category, and one might say puzzles in `hardest` are actually easier than `top95` counterparts. However, `top95` has 95 puzzles which makes it more prone to having outliers. `hardest` doesn't have enough puzzles to comment about their nature. Also, the mean time consumed to solve `hardest` puzzles is `r round(hardest_true_avg, 3)` seconds, which is close to `r round(top95_true_median, 3)` seconds, the median time consumed by solved puzzles of `top95` category. This implies that solved puzzles in `hardest` and `top95` are quite similar in nature.


## 2.2. How many puzzles required a brute force to be solved?

```{python result_sudsoln_report3, echo = F}
print(result_sudsoln_report3)
logically_total = result_sudsoln_report3.iloc[0, 0]
logically_max = result_sudsoln_report3.iloc[0, 4]
forcefully_total = result_sudsoln_report3.iloc[1, 0]
forcefully_min = result_sudsoln_report3.iloc[1, 1]
```

```{r result_sudsoln_report3_py, echo = F}
logically_total <- py$logically_total
logically_max <- py$logically_max
forcefully_total <- py$forcefully_total
forcefully_min <- py$forcefully_min
```

Out of `r all_is_solved` solved puzzles, `r logically_total` puzzles (`r round(logically_total / all_is_solved * 100, 3)`%) were solved by logical approaches only, and `r forcefully_total` puzzles (`r round(forcefully_total / all_is_solved * 100, 3)`%) required a brute force.

It is not surprising to see that puzzles were solved significantly faster when they were solely solved by logical approaches. This is quite obvious because `ss.Sudoku.solve()` first uses `ss.Sudoku.solve_logically()` and then `ss.Sudoku.solve_forcefully()` if logical approaches aren't enough. That is, every forcefully solved puzzle first went through `ss.Sudoku.solve_logically()` and then `ss.Sudoku.solve_forcefully()`. 

Interestingly, the minimum time consumed by one of forcefully solved puzzles (`r round(forcefully_min, 3)` seconds) is smaller than the maximum time consumed by one of logically solved puzzles (`r round(logically_max, 3)` seconds), implying that there is a case where a forceful solving mechanism was actually faster than a logical reasoning. One explanation is that those puzzles become solvable by logic after plugging in one candidate value to an entry. A puzzle of this type makes `ss.Sudoku.solve_logically()` to return a fairly small `ss.candidate.Candidate`, so a single selection of candidate value in `ss.Sudoku.solve_forcefully()` immediately leads to the answer form (or one of possibly many answer forms).

```{python result_sudsoln_report3_1, echo = F}
print(result_sudsoln_report3_1)
```

The following table lists forcefully solved puzzles that took less than `r round(logically_max, 3)` seconds, the maximum time consumed by one of logically solved puzzles. Notice that `trial`s are either 1 or 2, indicating that after plugging in one or two values to the puzzle's entries, [applying `ss.Sudoku.solve_logically()` inside `ss.Sudoku.solve_logically()`](https://joon3216.github.io/projects/2019/sudsoln/introduction.html#solve_forcefully) led to the answer.

```{python result_sudsoln_report3_2, echo = F}
print(result_sudsoln_report3_2)
count_ones = list(result_sudsoln_report3_2.trial).count(1)
max_time_ones = max(result_sudsoln_report3_2.loc[lambda x: x.trial == 1].time)
max_time_all = max(result_sudsoln_report3_2.time)
```

```{r result_sudsoln_report3_2_py, echo = F}
count_ones <- py$count_ones
max_time_ones <- py$max_time_ones
max_time_all <- py$max_time_all
```

This table shows forcefully solved puzzles that took longer than any other logically solved puzzles and yet took at most two attempts. We find that:

1. there are only `r count_ones` puzzles with `trial == 1` in this table, i.e. puzzles that took one attempt to solve are essentially the same as logically solved puzzles in terms of times consumed (They took no longer than `r ceiling(max_time_ones * (10^3)) / (10^3)` seconds). We may expect them to be solved completely by logic as the version increases.
2. puzzles solved within two attempts are all solved within `r ceiling(max_time_all * (10^3)) / (10^3)` seconds.



## 2.3. How many attempts did it take to solve a puzzle if forcefully solved?


```{python result_sudsoln_report4, echo = F}
print(result_sudsoln_report4)
forcefully_median_trial = result_sudsoln_report4.iloc[0, 2]
```

```{r result_sudsoln_report4_py, echo = F}
forcefully_median_trial <- py$forcefully_median_trial
```

(`total` in the table is the number of cases out of `r all_total` puzzles.)

The most apparent pattern is that both `time` and `trial` are extremely right-skewed regardless of the group. This shows that there is no "middle ground", i.e. the package either solved a puzzle fairly quickly, or it took a very long time to solve one. Thus, the median is more reliable than the mean. According to the above table, about `r forcefully_median_trial` trials are required whenever a puzzle requires a brute force to be solved.


```{python scatter1, fig.asp = .6, fig.align = 'center', echo = F}
# Scatterplot: time vs. trial
fig, ax = plt.subplots()
for solved_how in ['logically', 'forcefully', 'not_solved']:
    ax.scatter(
        x = result_sudsoln.loc[lambda x: x['solved'] == solved_how].trial,
        y = result_sudsoln.loc[lambda x: x['solved'] == solved_how].time,
        alpha = .2,
        label = solved_how
    )
ax.legend(title = 'How were they solved?')
ax.grid(True)
plt.xlabel('trial')
plt.ylabel('time (in sec)')
plt.title('Scatterplot of time vs. trial')
plt.show()
```

The scatterplot of `time` vs. `trial` shows that the vast majority of puzzles are solved within 50 trials or about a minute. That is, setting a big `max_trial` in `ss.Sudoku.solve()` (which leads to a longer running time) will increase a chance of a puzzle getting solved, but not by a large scale after `max_trial` gets greater than 50.

```{python line1_real, fig.asp = .6, fig.align = 'center', echo = F, results = 'hide'}
# Lineplot: prob vs. max_trial
prob_vs_max_trial = {'max_trial': [], 'prob': []}
numer = result_sudsoln.shape[0]
for attempt in range(max_trial + 1):
    denom = result_sudsoln\
        .loc[lambda x: (x['trial'] <= attempt) & (x['is_solved'])]\
        .total\
        .sum()
    prob_vs_max_trial['max_trial'].append(attempt)
    prob_vs_max_trial['prob'].append(denom / numer)
prob_vs_max_trial = pd.DataFrame(prob_vs_max_trial)

plt.clf()
plt.plot(prob_vs_max_trial.max_trial, prob_vs_max_trial.prob)
plt.ylim(0, 1) 
plt.yticks(np.arange(0, 1.1, .1))
plt.grid(True)
plt.xlabel('max_trial in ss.Sudoku.solve(max_trial)')
plt.ylabel('P(getting solved)')
plt.title('Expected P(getting solved) vs. max_trial')
plt.show()
```

This plot displays how the expected probability of a puzzle getting solved increases as `max_trial` argument specified in `ss.Sudoku.solve()` gets bigger. Notice the plateau starts after 50 `max_trial`, and stays around the probability of 90%.

To get the idea of how much more `time` is required as `trial` increases, I fit a linear regression model between log-transformed `time` and `trial`. In particular, the following model is used: $$\log(\text{time}) = \beta_0 + \beta_1 \log(\text{trial} + 1) + \varepsilon$$ Log-transformations are performed due to a severe right-skewness on both the response and the feature; `trial + 1` is used because whenever $\text{trial} = 0$, $\log(\text{trial})$ is undefined, and $\log(\text{trial} + 1)$ becomes exactly $0$. Model-checking procedure is skipped, and the model is assumed to be appropriate to describe the relationship between the two.

```{python scatter2, fig.asp = .6, fig.align = 'center', echo = F}
# Scatterplot: log(time) vs. log(trial + 1)
X = result_sudsoln.loc[:, ['total', 'trial']]
X.trial = np.log(X.trial + 1)
y = result_sudsoln.loc[:, ['time']]
y.time = np.log(y.time)
X, y = X.values, y.values
betas = np.linalg.inv(X.T @ X) @ X.T @ y
model_x = np.linspace(0, np.max(X[:, 1]))
model_y = betas[0] + betas[1] * model_x

plt.clf()
fig, ax = plt.subplots()
for solved_how in ['logically', 'forcefully', 'not_solved']:
    ax.scatter(
        x = np.log(
            result_sudsoln.loc[lambda x: x['solved'] == solved_how].trial + 1
        ),
        y = np.log(
            result_sudsoln.loc[lambda x: x['solved'] == solved_how].time
        ),
        alpha = .2,
        label = solved_how
    )
ax.legend(title = 'How were they solved?')
ax.grid(True)
plt.plot(model_x, model_y, alpha = .5)
plt.xlabel('log(trial + 1)')
plt.ylabel('log(time)')
plt.title('Scatterplot of log(time) vs. log(trial + 1)')
plt.show()
```

```{python scatter3_real, fig.asp = .6, fig.align = 'center', echo = F}
# time vs. trial according to the fitted model
model_time = lambda x: np.exp(betas[0] + betas[1] * np.log(x + 1))
model_trials = np.arange(201)
model_times = model_time(model_trials)

plt.clf()
fig, ax = plt.subplots()
for solved_how in ['logically', 'forcefully', 'not_solved']:
    ax.scatter(
        x = result_sudsoln.loc[lambda x: x['solved'] == solved_how].trial,
        y = result_sudsoln.loc[lambda x: x['solved'] == solved_how].time,
        alpha = .2,
        label = solved_how
    )
ax.legend(title = 'How were they solved?')
ax.grid(True)
plt.xlabel('trial')
plt.ylabel('time (in sec)')
plt.title('Scatterplot of time vs. trial with a regression line')
plt.plot(model_trials, model_times, alpha = .5)
plt.show()
```


```{r model_times_py, echo = F}
model_time_0 <- py$model_times[1]
model_time_50 <- py$model_times[51]
model_time_200 <- py$model_times[201]
```

According to the model:

* around 20% of sudoku puzzles with a `top95` or `hardest` difficulty are expected to be solved by `ss.Sudoku.solve_logically()` in about `r round(model_time_0, 3)` seconds
* around 90% of sudoku puzzles with a `top95` or `hardest` difficulty are expected to be solved by `ss.Sudoku.solve()` in about 50 attempts (trials), or `r round(model_time_50, 3)` seconds
* `ss.Sudoku.solve()` is expected to terminate in about `r round(model_time_200, 3)` seconds, regardless of whether the method solves a puzzle or not, if used the default setting of `max_trial = 200`


# Session info

```{python session_info}
import sinfo # ver 0.1.4
sinfo.sinfo()
```











































































